{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigs = torch.tensor((0, 1, 1, 2, 3, 4, 4, 4, 4))\n",
    "\n",
    "J = torch.diag(eigs) + torch.diag(torch.tensor([0, 1, 0, 0, 0, 1, 1, 1]), diagonal=1)\n",
    "J = J.float()\n",
    "X = torch.rand(J.shape)\n",
    "\n",
    "A = X @ J @ X.inverse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special formula for Jordan block form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from torch import func\n",
    "from math import factorial\n",
    "\n",
    "def jordan_form_func(function):\n",
    "    def split_jordan_blocks(matrix):\n",
    "        \"\"\"\n",
    "        Split a Jordan normal form matrix into its individual Jordan blocks.\n",
    "\n",
    "        Parameters:\n",
    "            matrix (torch.Tensor): The input square Jordan normal form matrix.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of tensors, each representing an individual Jordan block.\n",
    "        \"\"\"\n",
    "        n = matrix.size(0)\n",
    "        blocks = []\n",
    "        start_idx = 0\n",
    "\n",
    "        for i in range(n - 1):\n",
    "            if matrix[i, i + 1] != 1:  # End of a block\n",
    "                # Extract the block\n",
    "                blocks.append(matrix[start_idx:i + 1, start_idx:i + 1])\n",
    "                start_idx = i + 1\n",
    "\n",
    "        # Add the last block\n",
    "        if start_idx < n:\n",
    "            blocks.append(matrix[start_idx:n, start_idx:n])\n",
    "\n",
    "        return blocks\n",
    "\n",
    "    def grad(f, n=0):\n",
    "        return func.vmap(reduce(lambda f, _: torch.func.grad(f), range(n), f))\n",
    "\n",
    "    def wrapper(input):\n",
    "\n",
    "        input = input.reshape((1, 1)) if not input.dim() else input\n",
    "        assert input.size(0) == input.size(1), \"Input must be a square matrix\"\n",
    "\n",
    "        eigs = input.diagonal()\n",
    "\n",
    "        output = function(eigs).diag()\n",
    "\n",
    "        block_start_idx = 0\n",
    "        for block in split_jordan_blocks(input):\n",
    "            block_size = len(block)\n",
    "            eig = block.diagonal()\n",
    "            block_slice = slice(block_start_idx, block_start_idx+block_size)\n",
    "\n",
    "            for i in range(1, block_size):\n",
    "                output[block_slice, block_slice] += grad(function, i)(eig[:-i]).diag(diagonal=i) / factorial(i)\n",
    "\n",
    "            block_start_idx += block_size\n",
    "\n",
    "        return output\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000],\n",
       "        [ 0.0000,  0.4546, -0.4161,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000],\n",
       "        [ 0.0000,  0.0000,  0.4546,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000, -0.3784,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1397,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4947, -0.1455, -0.9894,\n",
       "          0.0970],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4947, -0.1455,\n",
       "         -0.9894],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4947,\n",
       "         -0.1455],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.4947]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jordan_form_func\n",
    "def f(x):\n",
    "    return torch.sin(x)*torch.cos(x)\n",
    "\n",
    "\n",
    "f(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taylor expansion for general matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taylor_extension(n_terms = 50, loc = 0.):\n",
    "    def compute_taylor(input, function):\n",
    "\n",
    "        input = input.reshape((1, 1)) if not input.dim() else input\n",
    "        assert input.size(0) == input.size(1), \"Input must be a square matrix\"\n",
    "\n",
    "        x_loc = loc * torch.ones((1, 1), requires_grad=True, dtype=torch.float64)\n",
    "        matrix_loc = x_loc * torch.eye(input.size(0), dtype=torch.float64)\n",
    "\n",
    "        factorial = torch.tensor([1.], dtype=torch.float64)\n",
    "        grad_fn = function(x_loc)\n",
    "        matrix_power = torch.eye(input.size(0), dtype=torch.float64)\n",
    "\n",
    "        output = grad_fn / factorial * matrix_power\n",
    "\n",
    "        for term in range(1, n_terms):\n",
    "            \n",
    "            grad_fn = torch.autograd.grad(grad_fn.sum(), x_loc, create_graph=True)[0]\n",
    "            factorial *= term\n",
    "            matrix_power @= input - matrix_loc \n",
    "\n",
    "            output += grad_fn / factorial * matrix_power\n",
    "\n",
    "        return output\n",
    "\n",
    "    def decorator(function):\n",
    "        def wrapper(input):\n",
    "            return compute_taylor(input=input, function=function)\n",
    "        return wrapper\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([\n",
    "    [10, 1, 0],\n",
    "    [0, -10, 1],\n",
    "    [0, 0, 10]\n",
    "], dtype=torch.float64)\n",
    "\n",
    "x = torch.tensor(10., dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(111.4385, dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.linalg.matrix_exp(A) - taylor_extension(20, 0)(torch.exp)(A)).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(taylor_extension(10, )(torch.exp)(x) - torch.exp(x)).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@taylor_extension(n_terms=50, loc=1)\n",
    "def h(x):\n",
    "    return torch.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2., dtype=torch.float64)\n",
    "\n",
    "h(x) - torch.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@taylor_extension(n_terms=50)\n",
    "def g(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "g(torch.tensor([[1.]])) - torch.exp(-torch.tensor([[1.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5440, -0.0544, -0.0392],\n",
       "        [ 0.0000,  0.5440, -0.0544],\n",
       "        [ 0.0000,  0.0000, -0.5440]], dtype=torch.float64,\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2356777.3223, dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.rand((100, 100), dtype=torch.float64)\n",
    "\n",
    "(torch.linalg.matrix_exp(B) - taylor_extension(180)(torch.exp)(B)).norm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firedrake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
