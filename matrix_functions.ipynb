{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigs = torch.tensor((0, 1, 1, 2, 3, 4, 4, 4, 4))\n",
    "\n",
    "J = torch.diag(eigs) + torch.diag(torch.tensor([0, 1, 0, 0, 0, 1, 1, 1]), diagonal=1)\n",
    "J = J.float()\n",
    "X = torch.rand(J.shape)\n",
    "\n",
    "A = X @ J @ X.inverse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special formula for Jordan block form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from torch import func\n",
    "from math import factorial\n",
    "\n",
    "def jordan_form_func(function):\n",
    "    def split_jordan_blocks(matrix):\n",
    "        \"\"\"\n",
    "        Split a Jordan normal form matrix into its individual Jordan blocks.\n",
    "\n",
    "        Parameters:\n",
    "            matrix (torch.Tensor): The input square Jordan normal form matrix.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of tensors, each representing an individual Jordan block.\n",
    "        \"\"\"\n",
    "        n = matrix.size(0)\n",
    "        blocks = []\n",
    "        start_idx = 0\n",
    "\n",
    "        for i in range(n - 1):\n",
    "            if matrix[i, i + 1] != 1:  # End of a block\n",
    "                # Extract the block\n",
    "                blocks.append(matrix[start_idx:i + 1, start_idx:i + 1])\n",
    "                start_idx = i + 1\n",
    "\n",
    "        # Add the last block\n",
    "        if start_idx < n:\n",
    "            blocks.append(matrix[start_idx:n, start_idx:n])\n",
    "\n",
    "        return blocks\n",
    "\n",
    "    def grad(f, n=0):\n",
    "        return func.vmap(reduce(lambda f, _: torch.func.grad(f), range(n), f))\n",
    "\n",
    "    def wrapper(input):\n",
    "\n",
    "        input = input.reshape((1, 1)) if not input.dim() else input\n",
    "        assert input.size(0) == input.size(1), \"Input must be a square matrix\"\n",
    "\n",
    "        eigs = input.diagonal()\n",
    "\n",
    "        output = function(eigs).diag()\n",
    "\n",
    "        block_start_idx = 0\n",
    "        for block in split_jordan_blocks(input):\n",
    "            block_size = len(block)\n",
    "            eig = block.diagonal()\n",
    "            block_slice = slice(block_start_idx, block_start_idx+block_size)\n",
    "\n",
    "            for i in range(1, block_size):\n",
    "                output[block_slice, block_slice] += grad(function, i)(eig[:-i]).diag(diagonal=i) / factorial(i)\n",
    "\n",
    "            block_start_idx += block_size\n",
    "\n",
    "        return output\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000],\n",
       "        [ 0.0000,  0.4546, -0.4161,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000],\n",
       "        [ 0.0000,  0.0000,  0.4546,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000, -0.3784,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1397,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4947, -0.1455, -0.9894,\n",
       "          0.0970],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4947, -0.1455,\n",
       "         -0.9894],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4947,\n",
       "         -0.1455],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.4947]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jordan_form_func\n",
    "def f(x):\n",
    "    return torch.sin(x)*torch.cos(x)\n",
    "\n",
    "\n",
    "f(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taylor expansion for general matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taylor_extension(n_terms = 30, loc = 0):\n",
    "    def decorator(function):\n",
    "        def wrapper(input):\n",
    "            input = input.reshape((1, 1)) if not input.dim() else input\n",
    "            assert input.size(0) == input.size(1), \"Input must be a square matrix\"\n",
    "\n",
    "            factorial = torch.tensor([1.], dtype=torch.float64)\n",
    "            x_loc = loc * torch.ones((1, 1), requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "            output = torch.zeros_like(input)\n",
    "            for term in range(n_terms):\n",
    "\n",
    "                grad_fn = torch.autograd.grad(grad_fn.sum(), x_loc, create_graph=True)[0] if term else function(x_loc)\n",
    "                \n",
    "                factorial = factorial*term if term else factorial*1\n",
    "\n",
    "                # (input - x_loc*torch.eye(inputs.size(0))) for loc other than 0\n",
    "                output += grad_fn / factorial * input.matrix_power(term)\n",
    "\n",
    "            return output\n",
    "\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5525e-11, dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([\n",
    "    [10, 1, 0],\n",
    "    [0, -10, 1],\n",
    "    [0, 0, 10]\n",
    "], dtype=torch.float64)\n",
    "\n",
    "\n",
    "(torch.linalg.matrix_exp(A) - taylor_extension(50)(torch.exp)(A)).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taylor_extension(100)(torch.sin)(torch.tensor(1.)) - torch.sin(torch.tensor(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3679]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@taylor_extension(n_terms=30)\n",
    "def g(x):\n",
    "    return torch.exp(-x)\n",
    "\n",
    "g(torch.tensor([[1.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7996e-03, -1.1013e+03,  5.5066e+01],\n",
       "        [ 0.0000e+00,  2.2026e+04, -1.1013e+03],\n",
       "        [ 0.0000e+00,  0.0000e+00, -2.7996e-03]], dtype=torch.float64,\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(295605.1250, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.rand((25, 25))\n",
    "\n",
    "(torch.linalg.matrix_exp(B) - taylor_extension(5)(torch.exp)(B)).norm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firedrake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
